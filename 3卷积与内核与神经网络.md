# 内核 -卷积 -神经网



## 卷积

例如制作一个内核，以便在对猫耳进行卷积操作时返回一个较高的值。因此，可能会有猫耳内核、猫鼻内核、猫眼内核……如果对一张图像运行所有这些内核并找到了匹配项，我们便可猜出图像中有猫！

但随着神经网络变得愈发强大，人们开始建立这种联系。内核其实是一种工具，可将权重与输入相乘并对结果求和。

神经元也可以将权重与输入相乘并对结果求和！

这正是卷积神经网络的功能。它们可以将神经元的输入映射到具有可训练权重的内核上。

// 有趣的历史：https://glassboxmedicine.com/2019/04/13/a-short-history-of-convolutional-neural-networks/

// 卷积神经网络 (CNN) 的反向传播工作原理：https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/



## 神经元，神经网络

Dropout 则是另一个有趣的工具，它在一定程度上反映了集成学习(Emsemble Learning)的思想。在训练过程中，我们将以指定的丢弃率随机关闭神经元，这意味着被关闭的神经元在相应的训练步骤中将无法进行学习。这可以防止过拟合，同时消除对任何特定输入或神经元的过度依赖。

注意不要将丢弃率设为 1！因为此操作会将所有神经元关闭，导致模型无法学习。

在验证或预测过程中，再将所有神经元重新打开。



我们还将执行批量归一化操作，在该操作过程中，我们会对训练过程中各层之间的权重变化量进行归一化。批量归一化是为防止内部协变量偏移，但事实证明，它并未真正做到这一点。即便如此，从经验来看，此方法似乎仍对目标检测网络有所帮助。